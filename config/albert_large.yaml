# ALBERT Large 모델 설정
dataset:
  name: "your_korean_dataset"  # 실제 데이터셋 이름으로 변경
  preprocessing_num_workers: 4
  overwrite_cache: false
  max_seq_length: 512

model:
  tokenizer_path: "albert-ko-tokenizer"  # 토크나이저 경로
  embedding_size: 128
  hidden_size: 1024
  num_hidden_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  # vocab_size: 자동으로 토크나이저에서 결정됨

training:
  output_dir: "./albert-ko-large-pretrained"
  per_device_train_batch_size: 4  # Large 모델은 메모리 요구량이 더 큼
  per_device_eval_batch_size: 4
  learning_rate: 5.0e-5
  weight_decay: 0.01
  max_steps: 1500000  # 실제 학습에 맞게 조정
  num_train_epochs: 3
  warmup_steps: 15000
  save_steps: 50000
  logging_steps: 1000
  eval_steps: 50000
  gradient_accumulation_steps: 2  # 더 큰 유효 배치 크기를 위해
  save_total_limit: 5
  seed: 42
  fp16: true  # 메모리 효율성을 위해 FP16 사용
  local_rank: -1
