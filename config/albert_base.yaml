# ALBERT Base 모델 설정
dataset:
  name: "your_korean_dataset"  # 실제 데이터셋 이름으로 변경
  preprocessing_num_workers: 4
  overwrite_cache: false
  max_seq_length: 512

model:
  tokenizer_path: "albert-ko-tokenizer"  # 토크나이저 경로
  embedding_size: 128
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  # vocab_size: 자동으로 토크나이저에서 결정됨

training:
  output_dir: "./albert-ko-pretrained"
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  learning_rate: 1.0e-4
  weight_decay: 0.01
  max_steps: 1000000  # 실제 학습에 맞게 조정
  num_train_epochs: 3
  warmup_steps: 10000
  save_steps: 50000
  logging_steps: 1000
  eval_steps: 50000
  gradient_accumulation_steps: 1
  save_total_limit: 5
  seed: 42
  fp16: false
  local_rank: -1
